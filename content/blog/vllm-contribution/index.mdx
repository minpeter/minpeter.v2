---
title: vLLM 오픈소스 기여 후기
description: PR 제출부터 머지까지, 4개월간의 vLLM 기여 과정
drafted: 2025-08-19
published: 2026-01-21
---

## 개요

vLLM은 LLM 추론을 위한 대표적인 오픈소스 프로젝트입니다. 작년에 Hermes2ProToolParser의 개선 사항을 [PR #16890](https://github.com/vllm-project/vllm/pull/16890)로 기여하여 메인 브랜치에 머지되었습니다.

이 글에서는 기여하게 된 배경과 구현 내용, 그리고 오픈소스 기여 과정에서 배운 점을 공유합니다.

## 문제 상황

vLLM의 기존 Hermes2ProToolParser는 `<tool_call>`과 `</tool_call>` 태그가 토크나이저 상에서 **별도의 special token**으로 정의된 모델에서만 정상 동작했습니다.
_(e.g., [NousResearch/Hermes-3-Llama-3.1-8B](https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B/blob/main/tokenizer_config.json), llama 3 토크나이저를 편집하여 128002, 128013 토큰을 도구 호출용으로 할당함.)_

하지만 Llama와 같은 모델을 별도의 토크나이저 편집 없이 Hermes 포맷으로 fine-tuning하면, 이 태그들이 special token이 아닌 일반 텍스트로 토큰화됩니다. 예를 들어:

```
"<tool_call>" → ["<", "tool", "_", "call", ">"]
```

이렇게 여러 토큰으로 분리되면 기존 파서가 tool call을 제대로 감지하지 못하는 문제가 발생했습니다.

물론 NousResearch에서 한 것처럼 reserved_special_token 의 일부를 편집하여 별도의 도구 호출 용도의 special token를 할당한 후 학습할 수도 있지만, 저의 자체적인 모델 학습 실험에 따르면 Full Fine-tuning이 아닌 LoRA 같은 학습 방법을 활용한 경우, 추가된 special token를 충분히 학습하지 못하고 special token를 추가하지 않고 학습한 모델 대비 벤치마크 성능이 하락하는 모습을 보여줬습니다.

## 해결 방법

이 문제를 해결하기 위해 vllm의 [Tool Parser Plugin](https://docs.vllm.ai/en/latest/features/tool_calling/#how-to-write-a-tool-parser-plugin) 기능을 활용하여, [minpeter/hermes-llama-parse](https://github.com/minpeter/hermes-llama-parse) 레포에서 먼저 프로토타입을 구현했습니다.

핵심 아이디어는 **버퍼링 메커니즘**을 추가하는 것입니다:

1. 스트리밍 출력에서 `<`로 시작하는 부분 문자열을 감지
2. `<tool_call>` 또는 `</tool_call>`이 완성될 때까지 버퍼에 저장
3. 완성되면 tool call로 파싱, 아니면 일반 텍스트로 출력

```python
# 버퍼링 로직 핵심 부분
if current_text.endswith("<") or self._buffer:
    self._buffer += delta_text
    if self._buffer.startswith("<tool_call>"):
        # tool call 시작 감지
        ...
```

## vLLM 기여 과정

### 1. PR 제출 (4월 20일)

hermes-llama-parse에서 검증된 구현을 vLLM에 PR로 제출했습니다.

### 2. 코드 리뷰 (6월)

vLLM 메인테이너 [@aarnphm](https://github.com/aarnphm)으로부터 테스트 케이스 추가 요청을 받았습니다.

> "Is there a test fine-tuned model that we can use to test this?"

테스트를 위해 직접 fine-tuning한 모델 [minpeter/LoRA-Llama-3.2-1B-tool-vllm-ci](https://huggingface.co/minpeter/LoRA-Llama-3.2-1B-tool-vllm-ci)를 사용한 e2e 테스트를 작성했습니다.

### 3. 머지 (8월 16일)

약 4개월간의 리뷰 과정을 거쳐 최종 머지되었습니다.

## 배운 점

### 오픈소스 기여는 인내가 필요하다

PR 제출부터 머지까지 약 4개월이 걸렸습니다. 대형 오픈소스 프로젝트는 많은 PR이 쌓여있고, 메인테이너들도 바쁘기 때문에 시간이 걸릴 수 있습니다.

### 테스트의 중요성

"It works on my machine"은 충분하지 않습니다. 재현 가능한 테스트 케이스와 테스트용 모델을 제공해야 리뷰어가 검증할 수 있습니다.

### 작은 것부터 시작하기

처음부터 대규모 변경을 시도하기보다, 별도 레포에서 프로토타입을 만들어 검증한 후 업스트림하는 방식이 효과적이었습니다.

## 마무리

이번 기여를 통해 vLLM에서 Hermes 포맷으로 fine-tuning된 Llama 기반 모델들의 tool calling이 정상 동작하게 되었습니다.

오픈소스 기여는 생각보다 어렵지 않습니다. 사용하면서 불편한 점을 발견하면, 그것이 곧 기여 기회입니다.

## 관련 링크

- [vLLM PR #16890](https://github.com/vllm-project/vllm/pull/16890)
- [hermes-llama-parse (원본 구현)](https://github.com/minpeter/hermes-llama-parse)
- [테스트 모델 (HuggingFace)](https://huggingface.co/minpeter/LoRA-Llama-3.2-1B-tool-vllm-ci)
