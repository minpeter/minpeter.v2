---
title: vLLM 오픈소스 기여 후기
description: PR 제출부터 머지까지, 4개월간의 vLLM 기여 과정
drafted: 2025-08-19
published: 2026-01-21
---

## 개요

vLLM은 LLM 추론을 위한 대표적인 오픈소스 프로젝트입니다. 작년에 Hermes2ProToolParser의 개선 사항을 [PR #16890](https://github.com/vllm-project/vllm/pull/16890)로 기여하여 메인 브랜치에 머지되었습니다.

이 글에서는 기여하게 된 배경과 구현 내용, 그리고 오픈소스 기여 과정에서 배운 점을 공유합니다.

## 문제 상황

vLLM의 기존 Hermes2ProToolParser는 `<tool_call>`과 `</tool_call>` 태그가 토크나이저 상에서 **별도의 special token**으로 정의된 모델에서만 정상 동작했습니다.
_(e.g., [NousResearch/Hermes-3-Llama-3.1-8B](https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B/blob/main/tokenizer_config.json), llama 3 토크나이저를 편집하여 128002, 128013 토큰을 도구 호출용으로 할당함.)_

하지만 Llama와 같은 모델을 별도의 토크나이저 편집 없이 Hermes 포맷으로 fine-tuning하면, 이 태그들이 special token이 아닌 일반 텍스트로 토큰화됩니다. 예를 들어:

```
"<tool_call>" → ["<", "tool", "_", "call", ">"]
```

이렇게 여러 토큰으로 분리되면 기존 파서가 tool call을 제대로 감지하지 못하는 문제가 발생했습니다.

물론 NousResearch에서 한 것처럼 reserved_special_token 의 일부를 편집하여 별도의 도구 호출 용도의 special token를 할당한 후 학습할 수도 있지만, 저의 자체적인 모델 학습 실험에 따르면 Full Fine-tuning이 아닌 LoRA 같은 학습 방법을 활용한 경우, 추가된 special token를 충분히 학습하지 못하고 special token를 추가하지 않고 학습한 모델 대비 벤치마크 성능이 하락하는 모습을 보여줬습니다.

## 해결 방법

이 문제를 해결하기 위해 vllm의 [Tool Parser Plugin](https://docs.vllm.ai/en/latest/features/tool_calling/#how-to-write-a-tool-parser-plugin) 기능을 활용하여, [minpeter/hermes-llama-parse](https://github.com/minpeter/hermes-llama-parse) 레포에서 먼저 프로토타입을 구현했습니다.

핵심 아이디어는 **버퍼링 메커니즘**을 추가하는 것입니다:

1. 스트리밍 출력에서 `<`로 시작하는 부분 문자열을 감지
2. `<tool_call>` 또는 `</tool_call>`이 완성될 때까지 버퍼에 저장
3. 완성되면 tool call로 파싱, 아니면 일반 텍스트로 출력

```python
def tool_call_delta_buffer(self, delta_text: str):
    if (delta_text in self.tool_call_start_token_array
            or delta_text in self.tool_call_end_token_array):
        if (delta_text == self.tool_call_start_token_array[-1]
                or delta_text == self.tool_call_end_token_array[-1]):
            buffered_text = self.buffered_delta_text
            self.buffered_delta_text = ""
            return buffered_text + delta_text
        else:
            self.buffered_delta_text = self.buffered_delta_text + delta_text
            return ""
    else:
        if self.buffered_delta_text:
            buffered_text = self.buffered_delta_text
            self.buffered_delta_text = ""
            return buffered_text + delta_text
        else:
            return delta_text
```

## vLLM 기여 과정

### 1. PR 제출 (4월 20일)

hermes-llama-parse에서 검증된 구현을 vLLM에 PR로 제출했습니다.

### 2. 코드 리뷰 (6월)

vLLM 메인테이너 [@aarnphm](https://github.com/aarnphm)으로부터 테스트 케이스 추가 요청을 받았습니다.

> "Is there a test fine-tuned model that we can use to test this?"

테스트를 위해 직접 fine-tuning한 모델 [minpeter/LoRA-Llama-3.2-1B-tool-vllm-ci](https://huggingface.co/minpeter/LoRA-Llama-3.2-1B-tool-vllm-ci)를 사용한 e2e 테스트를 작성했습니다.

처음 테스트를 작성할때 기존에 학습 해뒀던 3b 모델 [minpeter/m-3b-v1-iteration-00-sf-xlam-09](https://huggingface.co/minpeter/m-3b-v1-iteration-00-sf-xlam-09)를 이용해서 e2e 테스트를 작성하였는데, 뭔가 이후에 바로 리뷰를 해주시진 않아서 "maybe a very small finetune llama3.2-1b would work here." 라고 언급한 내용이 기억나 이것 때문에 안되나 하는 마음에 llama 3.2-1b 기반으로 모델을 학습시켜서 [교체](https://github.com/vllm-project/vllm/pull/16890/changes/e3a7d323c3d495f3d4bba8506c0223ab877bf723)했던 기억이 납니다.

### 3. 머지 (8월 16일)

약 4개월간의 기다림을 거쳐 최종 머지되었습니다.

approved 이후에 메인테이너분이 PR에 대해서 auto-merge 를 켜주셨는데, 당시에 업스트림에 문제로 인해서 몇가지 CI가 실패하던 상황이였고, 머지를 위해서는 전부 pass를 받아야했는데, 이걸 재시도할 방법이 없어서 main branch에 커밋 생길때마다 update this branch 를 누르면서 제발 성공해라 기도를 한 3트 하고 겨우 머지 되었습니다.

## 배운 점

### 오픈소스 기여는 인내가 필요하다

PR 제출부터 머지까지 약 4개월이 걸렸습니다. 대형 오픈소스 프로젝트는 많은 PR이 쌓여있고, 메인테이너들도 바쁘기 때문에 시간이 걸릴 수 있습니다.
생각 이상으로 더 느긋한 마음 가짐으로 기다리는 것이 중요한 것 같습니다.

물론 저는 기다림과는 거리가 멀어서 vllm slack까지 찾아가서 #feat-tool-calling 체널에서 리뷰해줘를 외치긴 했지만,, 이게 도움이 됐는지는 사실 잘 모르겠습니다.

### READ CONTRIBUTING.md

보통 여기에 적힌 글은 PR 올리기 전에 정독하고 뭔가 하는게 좋겠다는 생각이 들었습니다.
사소하게는 PR title 컨벤션, DCO 동의부터 린팅 같은 디테일까지, 리뷰어가 저와 같은 snake case 컨벤션도 안지킨 PR을 하루에 몇십개씩 보면 무슨 마음일지 생각하면서 코드를 한번 더 검토해보면 모두가 행복한 세상이 될 거 같다는 생각이 들었습니다.

## 마무리

이번 기여를 통해 vLLM에서 Hermes 포맷으로 fine-tuning된 Llama 기반 모델들의 tool calling이 정상 동작하게 되었습니다.

오픈소스 기여는 생각보다 어렵지 않습니다. 사용하면서 불편한 점을 발견하면, 그것이 곧 기여 기회입니다.

## 관련 링크

- [vLLM PR #16890](https://github.com/vllm-project/vllm/pull/16890)
- [hermes-llama-parse (원본 구현)](https://github.com/minpeter/hermes-llama-parse)
- [테스트 모델 (HuggingFace)](https://huggingface.co/minpeter/LoRA-Llama-3.2-1B-tool-vllm-ci)
